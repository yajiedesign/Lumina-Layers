"""
Lumina Studio - Image Processing Core

Handles image loading, preprocessing, color quantization and matching.
"""

import numpy as np
import cv2
from PIL import Image
from scipy.spatial import KDTree

from config import PrinterConfig, ModelingMode

# SVG support (optional dependency)
try:
    from svglib.svglib import svg2rlg
    from reportlab.graphics import renderPM
    HAS_SVG = True
except ImportError:
    HAS_SVG = False
    print("âš ï¸ [SVG] svglib/reportlab not installed. SVG support disabled.")


class LuminaImageProcessor:
    """
    Image processor class.
    
    Handles LUT loading, image processing, and color matching.
    """
    
    def __init__(self, lut_path, color_mode):
        """
        Initialize image processor.
        
        Args:
            lut_path: LUT file path (.npy)
            color_mode: Color mode string (CMYW/RYBW/6-Color)
        """
        self.color_mode = color_mode
        self.lut_rgb = None
        self.ref_stacks = None
        self.kdtree = None
        
        self._load_lut(lut_path)
    
    def _load_svg(self, svg_path, target_width_mm):
        """
        [Final Fix] Safe Padding + Dual-Pass Transparency Detection.
        
        Method: Render twice (White BG / Black BG).
        - If pixel changes color -> It's background (Transparent) -> Remove it.
        - If pixel stays same -> It's content (Opaque) -> Keep it 100% intact.
        
        This guarantees NO internal image damage.
        """
        if not HAS_SVG:
            raise ImportError("Please install 'svglib' and 'reportlab'.")
        
        print(f"[SVG] Rasterizing: {svg_path}")
        
        # 1. è¯»å– SVG
        drawing = svg2rlg(svg_path)
        
        # --- æ­¥éª¤ A: æ’‘å¤§ç”»å¸ƒ (ç¡®ä¿å†…å®¹ä¸è¢«åˆ‡æ–­) ---
        x1, y1, x2, y2 = drawing.getBounds()
        raw_w = x2 - x1
        raw_h = y2 - y1
        
        # æ·»åŠ  20% å®‰å…¨è¾¹è·
        padding_x = raw_w * 0.2
        padding_y = raw_h * 0.2
        
        drawing.translate(-x1 + padding_x, -y1 + padding_y)
        drawing.width = raw_w + (padding_x * 2)
        drawing.height = raw_h + (padding_y * 2)
        
        # 2. ç¼©æ”¾
        pixels_per_mm = 20.0
        target_width_px = int(target_width_mm * pixels_per_mm)
        
        if raw_w > 0:
            scale_factor = target_width_px / raw_w
        else:
            scale_factor = 1.0
        
        drawing.scale(scale_factor, scale_factor)
        drawing.width = int(drawing.width * scale_factor)
        drawing.height = int(drawing.height * scale_factor)
        
        # ================== ã€ç»ˆææ–¹æ¡ˆã€‘åŒé‡æ¸²æŸ“å·®åˆ†æ³• ==================
        try:
            # Pass 1: ç™½åº•æ¸²æŸ“ (0xFFFFFF)
            # å¼ºåˆ¶ä¸ä½¿ç”¨é€æ˜é€šé“ï¼Œå®Œå…¨æ¨¡æ‹Ÿæ‰“å°åœ¨ç™½çº¸ä¸Šçš„æ•ˆæœ
            pil_white = renderPM.drawToPIL(drawing, bg=0xFFFFFF, configPIL={'transparent': False})
            arr_white = np.array(pil_white.convert('RGB'))  # ä¸¢å¼ƒ Alphaï¼Œåªçœ‹é¢œè‰²
            
            # Pass 2: é»‘åº•æ¸²æŸ“ (0x000000)
            # å¼ºåˆ¶ä¸ä½¿ç”¨é€æ˜é€šé“ï¼Œå®Œå…¨æ¨¡æ‹Ÿæ‰“å°åœ¨é»‘çº¸ä¸Šçš„æ•ˆæœ
            pil_black = renderPM.drawToPIL(drawing, bg=0x000000, configPIL={'transparent': False})
            arr_black = np.array(pil_black.convert('RGB'))
            
            # è®¡ç®—å·®å¼‚ (Difference)
            # diff = |ç™½åº•å›¾ - é»‘åº•å›¾|
            # å¦‚æœåƒç´ æ˜¯å®å¿ƒçš„ï¼Œå®ƒæŒ¡ä½äº†èƒŒæ™¯ï¼Œæ‰€ä»¥åœ¨ç™½åº•å’Œé»‘åº•ä¸Šé¢œè‰²ä¸€æ · -> diff ä¸º 0
            # å¦‚æœåƒç´ æ˜¯é€æ˜çš„ï¼Œå®ƒé€å‡ºäº†èƒŒæ™¯ï¼Œæ‰€ä»¥åœ¨ç™½åº•æ˜¯ç™½ï¼Œé»‘åº•æ˜¯é»‘ -> diff å¾ˆå¤§
            diff = np.abs(arr_white.astype(int) - arr_black.astype(int))
            diff_sum = np.sum(diff, axis=2)
            
            # ç”Ÿæˆå®Œç¾çš„ Alpha æ©è†œ
            # åªè¦å·®å¼‚å°äº 10ï¼Œæˆ‘ä»¬å°±è®¤ä¸ºå®ƒæ˜¯å®å¿ƒå†…å®¹ (å®¹é”™å¤„ç†æŠ—é”¯é½¿è¾¹ç¼˜)
            # è¿™æ ·ç»å¯¹ä¸ä¼šè¯¯ä¼¤å›¾åƒå†…éƒ¨çš„ä»»ä½•é¢œè‰²
            alpha_mask = np.where(diff_sum < 10, 255, 0).astype(np.uint8)
            
            # åˆæˆæœ€ç»ˆå›¾åƒ
            # æˆ‘ä»¬å–ç™½åº•å›¾çš„é¢œè‰² (å› ä¸ºå®ƒæ˜¯å®å¿ƒçš„ï¼Œå–é»‘åº•å›¾ä¹Ÿä¸€æ ·)ï¼Œç„¶åæŠŠç®—å‡ºæ¥çš„ alpha è´´ä¸Šå»
            r, g, b = cv2.split(arr_white)
            img_final = cv2.merge([r, g, b, alpha_mask])
            
            # æ‰§è¡Œå®‰å…¨è£åˆ‡
            coords = cv2.findNonZero(alpha_mask)
            
            if coords is not None:
                x, y, w_rect, h_rect = cv2.boundingRect(coords)
                
                if w_rect > 0 and h_rect > 0:
                    print(f"[SVG] Dual-Pass Crop: {w_rect}x{h_rect} (Safe & Clean)")
                    
                    # ç•™ 2 åƒç´ è¾¹ç¼˜
                    pad = 2
                    y_start = max(0, y - pad)
                    y_end = min(img_final.shape[0], y + h_rect + pad)
                    x_start = max(0, x - pad)
                    x_end = min(img_final.shape[1], x + w_rect + pad)
                    
                    img_final = img_final[y_start:y_end, x_start:x_end]
                else:
                    print("[SVG] Warning: Content too small.")
            else:
                print("[SVG] Warning: Image appears fully transparent.")
            
            print(f"[SVG] Final resolution: {img_final.shape[1]}x{img_final.shape[0]} px")
            return img_final
            
        except Exception as e:
            print(f"[SVG] Dual-Pass failed: {e}")
            import traceback
            traceback.print_exc()
            
            # æœ€åçš„ä¿åº•ï¼šå¦‚æœåŒé‡æ¸²æŸ“å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ¸²æŸ“
            pil_img = renderPM.drawToPIL(drawing, bg=None, configPIL={'transparent': True})
            return np.array(pil_img.convert('RGBA'))
    
    def _load_lut(self, lut_path):
        """
        Load and validate LUT file (Supports 4-Color and 6-Color).
        
        Automatically detects LUT type based on size:
        - 1024 colors: 4-Color Standard (CMYW/RYBW)
        - 1296 colors: 6-Color Smart 1296
        """
        try:
            lut_grid = np.load(lut_path)
            measured_colors = lut_grid.reshape(-1, 3)
            total_colors = measured_colors.shape[0]
        except Exception as e:
            raise ValueError(f"âŒ LUT file corrupted: {e}")
        
        valid_rgb = []
        valid_stacks = []
        
        print(f"[IMAGE_PROCESSOR] Loading LUT with {total_colors} points...")
        
        # Branch 1: 6-Color Smart 1296
        if "6-Color" in self.color_mode or total_colors == 1296:
            print("[IMAGE_PROCESSOR] Detected 6-Color Smart 1296 mode")
            
            from core.calibration import get_top_1296_colors
            
            # Retrieve 1296 intelligent stacking order (must match calibration.py logic)
            # Note: generate_smart_board uses padding to fill 38x38,
            # but extractor extracts the border-removed 36x36 (1296 cells).
            # So we directly get the original stacking data here.
            
            smart_stacks = get_top_1296_colors()
            
            # Reverse stacking order to make it (Top -> Bottom)
            # Original smart_stacks is [Bottom, ..., Top] (simulation data order)
            # But Converter's Face Down logic prints Z=0 as Index=0
            # So we need to reverse to [Top, ..., Bottom], making Z=0 the viewing surface
            smart_stacks = [tuple(reversed(s)) for s in smart_stacks]
            print("[IMAGE_PROCESSOR] Stacks reversed for Face-Down printing compatibility.")
            
            if len(smart_stacks) != total_colors:
                print(f"âš ï¸ Warning: Stacks count ({len(smart_stacks)}) != LUT count ({total_colors})")
                min_len = min(len(smart_stacks), total_colors)
                smart_stacks = smart_stacks[:min_len]
                measured_colors = measured_colors[:min_len]
            
            # No "Base Blue" filtering in 6-color mode (colors too complex)
            self.lut_rgb = measured_colors
            self.ref_stacks = np.array(smart_stacks)
            
            print(f"âœ… LUT loaded: {len(self.lut_rgb)} colors (6-Color mode)")
        
        # Branch 2: 4-Color Standard (1024)
        else:
            print("[IMAGE_PROCESSOR] Detected 4-Color Standard mode")
            
            # Keep original outlier filtering logic (Blue Check)
            base_blue = np.array([30, 100, 200])
            dropped = 0
            
            for i in range(1024):
                if i >= total_colors:
                    break
                
                # Rebuild 4-base stacking (0..1023)
                digits = []
                temp = i
                for _ in range(5):
                    digits.append(temp % 4)
                    temp //= 4
                stack = digits[::-1]
                
                real_rgb = measured_colors[i]
                
                # Filter outliers: close to blue but doesn't contain blue
                dist = np.linalg.norm(real_rgb - base_blue)
                if dist < 60 and 3 not in stack:  # 3 is Blue in RYBW/CMYW
                    dropped += 1
                    continue
                
                valid_rgb.append(real_rgb)
                valid_stacks.append(stack)
            
            self.lut_rgb = np.array(valid_rgb)
            self.ref_stacks = np.array(valid_stacks)
            
            print(f"âœ… LUT loaded: {len(self.lut_rgb)} colors (filtered {dropped} outliers)")
        
        # Build KD-Tree
        self.kdtree = KDTree(self.lut_rgb)
    
    def process_image(self, image_path, target_width_mm, modeling_mode,
                     quantize_colors, auto_bg, bg_tol,
                     blur_kernel=0, smooth_sigma=10):
        """
        Main image processing method
        
        Args:
            image_path: Image file path
            target_width_mm: Target width (millimeters)
            modeling_mode: Modeling mode ("high-fidelity", "pixel")
            quantize_colors: K-Means quantization color count
            auto_bg: Whether to auto-remove background
            bg_tol: Background tolerance
            blur_kernel: Median filter kernel size (0=disabled, recommended 0-5)
            smooth_sigma: Bilateral filter sigma value (recommended 5-20)
        
        Returns:
            dict: Dictionary containing processing results
                - matched_rgb: (H, W, 3) Matched RGB array
                - material_matrix: (H, W, Layers) Material index matrix
                - mask_solid: (H, W) Solid mask
                - dimensions: (width, height) Pixel dimensions
                - pixel_scale: mm/pixel ratio
                - mode_info: Mode information dictionary
                - debug_data: Debug data (high-fidelity mode only)
        """
        print(f"[IMAGE_PROCESSOR] Mode: {modeling_mode.get_display_name()}")
        print(f"[IMAGE_PROCESSOR] Filter settings: blur_kernel={blur_kernel}, smooth_sigma={smooth_sigma}")
        
        # ========== Image Loading Logic Branch ==========
        is_svg = image_path.lower().endswith('.svg')
        
        if is_svg:
            print("[IMAGE_PROCESSOR] SVG detected - Engaging Ultra-High-Fidelity Vector Mode")
            img_arr = self._load_svg(image_path, target_width_mm)
            # SVG reset to PIL object to reuse subsequent logic (e.g., get dimensions)
            img = Image.fromarray(img_arr)
            
            # [CRITICAL] SVG is also a type of High-Fidelity, but it doesn't need denoising
            # Force override filter parameters, because vector graphics have no noise, no need to blur
            # 
            # [SUPER-SAMPLING STRATEGY]
            # We render at 20 px/mm (2x standard), which physically eliminates jaggies
            # through super-sampling. This is superior to blur-based anti-aliasing
            # because it preserves sharp edges while making curves smooth.
            blur_kernel = 0
            smooth_sigma = 0
            print("[IMAGE_PROCESSOR] SVG Mode: Filters disabled (Vector source is clean)")
            print("[IMAGE_PROCESSOR] Super-sampling at 20 px/mm eliminates jagged edges naturally")
            
            # Recalculate target_w/h (based on rendered dimensions)
            target_w, target_h = img.size
            pixel_to_mm_scale = 0.05  # 20 px/mm (1/20) - Ultra-High-Fidelity
        else:
            # [Original Logic] Bitmap loading
            # Load image
            img = Image.open(image_path).convert('RGBA')
            
            # DEBUG: Check original image properties
            print(f"[IMAGE_PROCESSOR] Original image: {image_path}")
            print(f"[IMAGE_PROCESSOR] Image mode: {Image.open(image_path).mode}")
            print(f"[IMAGE_PROCESSOR] Image size: {Image.open(image_path).size}")
            
            # Check if image has transparency
            original_img = Image.open(image_path)
            has_alpha = original_img.mode in ('RGBA', 'LA') or (original_img.mode == 'P' and 'transparency' in original_img.info)
            print(f"[IMAGE_PROCESSOR] Has alpha channel: {has_alpha}")
            
            if has_alpha:
                # Check alpha channel statistics
                if original_img.mode != 'RGBA':
                    original_img = original_img.convert('RGBA')
                alpha_data = np.array(original_img)[:, :, 3]
                print(f"[IMAGE_PROCESSOR] Alpha stats: min={alpha_data.min()}, max={alpha_data.max()}, mean={alpha_data.mean():.1f}")
                print(f"[IMAGE_PROCESSOR] Transparent pixels (alpha<10): {np.sum(alpha_data < 10)}")
            
            # Calculate target resolution
            if modeling_mode == ModelingMode.HIGH_FIDELITY:
                # High-precision mode: 10 pixels/mm
                PIXELS_PER_MM = 10
                target_w = int(target_width_mm * PIXELS_PER_MM)
                pixel_to_mm_scale = 1.0 / PIXELS_PER_MM  # 0.1 mm per pixel
                print(f"[IMAGE_PROCESSOR] High-res mode: {PIXELS_PER_MM} px/mm")
            else:
                # Pixel mode: Based on nozzle width
                target_w = int(target_width_mm / PrinterConfig.NOZZLE_WIDTH)
                pixel_to_mm_scale = PrinterConfig.NOZZLE_WIDTH
                print(f"[IMAGE_PROCESSOR] Pixel mode: {1.0/pixel_to_mm_scale:.2f} px/mm")
            
            target_h = int(target_w * img.height / img.width)
            print(f"[IMAGE_PROCESSOR] Target: {target_w}Ã—{target_h}px ({target_w*pixel_to_mm_scale:.1f}Ã—{target_h*pixel_to_mm_scale:.1f}mm)")
        
        # ========== End of Image Loading Logic Branch ==========
        
        # ========== CRITICAL FIX: Use NEAREST for both modes ==========
        # REASON: LANCZOS anti-aliasing creates light transition pixels at edges.
        # These light pixels map to stacks with WHITE bases (Layer 1),
        # causing the mesh to "float" above the build plate.
        # 
        # SOLUTION: Use NEAREST to preserve hard edges and ensure dark pixels
        # map to solid dark stacks from Layer 1 upwards.
        print(f"[IMAGE_PROCESSOR] Using NEAREST interpolation (no anti-aliasing)")
        img = img.resize((target_w, target_h), Image.Resampling.NEAREST)
        
        img_arr = np.array(img)
        rgb_arr = img_arr[:, :, :3]
        alpha_arr = img_arr[:, :, 3]
        
        # CRITICAL FIX: Identify transparent pixels BEFORE color processing
        # This prevents transparent areas from being matched to LUT colors
        mask_transparent_initial = alpha_arr < 10
        print(f"[IMAGE_PROCESSOR] Found {np.sum(mask_transparent_initial)} transparent pixels (alpha<10)")
        
        # Color processing and matching
        debug_data = None
        if modeling_mode == ModelingMode.HIGH_FIDELITY:
            matched_rgb, material_matrix, bg_reference, debug_data = self._process_high_fidelity_mode(
                rgb_arr, target_h, target_w, quantize_colors, blur_kernel, smooth_sigma
            )
        else:
            matched_rgb, material_matrix, bg_reference = self._process_pixel_mode(
                rgb_arr, target_h, target_w
            )
        
        # Background removal - combine alpha transparency with optional auto-bg
        mask_transparent = mask_transparent_initial.copy()
        if auto_bg:
            bg_color = bg_reference[0, 0]
            diff = np.sum(np.abs(bg_reference - bg_color), axis=-1)
            mask_transparent = np.logical_or(mask_transparent, diff < bg_tol)
        
        # Apply transparency mask to material matrix
        material_matrix[mask_transparent] = -1
        mask_solid = ~mask_transparent
        
        result = {
            'matched_rgb': matched_rgb,
            'material_matrix': material_matrix,
            'mask_solid': mask_solid,
            'dimensions': (target_w, target_h),
            'pixel_scale': pixel_to_mm_scale,
            'mode_info': {
                'mode': modeling_mode
            }
        }
        
        # Add debug data (high-fidelity mode only)
        if debug_data is not None:
            result['debug_data'] = debug_data
        
        return result

    
    def _process_high_fidelity_mode(self, rgb_arr, target_h, target_w, quantize_colors,
                                    blur_kernel, smooth_sigma):
        """
        High-fidelity mode image processing
        Includes configurable filtering, K-Means quantization and color matching
        
        ä¼˜åŒ–ï¼š
        1. K-Means++ åˆå§‹åŒ–ï¼ˆOpenCV é»˜è®¤æ”¯æŒï¼‰
        2. é¢„ç¼©æ”¾ï¼šåœ¨å°å›¾ä¸Šåš K-Meansï¼Œç„¶åæ˜ å°„å›åŸå›¾
        
        Args:
            rgb_arr: Input RGB array
            target_h: Target height
            target_w: Target width
            quantize_colors: K-Means color count
            blur_kernel: Median filter kernel size (0=disabled)
            smooth_sigma: Bilateral filter sigma value
        
        Returns:
            tuple: (matched_rgb, material_matrix, quantized_image, debug_data)
        """
        import time
        total_start = time.time()
        
        print(f"[IMAGE_PROCESSOR] Starting edge-preserving processing...")
        
        # Step 1: Bilateral filter (edge-preserving smoothing)
        t0 = time.time()
        if smooth_sigma > 0:
            print(f"[IMAGE_PROCESSOR] Applying bilateral filter (sigma={smooth_sigma})...")
            rgb_processed = cv2.bilateralFilter(
                rgb_arr.astype(np.uint8), 
                d=9,
                sigmaColor=smooth_sigma, 
                sigmaSpace=smooth_sigma
            )
        else:
            print(f"[IMAGE_PROCESSOR] Bilateral filter disabled (sigma=0)")
            rgb_processed = rgb_arr.astype(np.uint8)
        print(f"[IMAGE_PROCESSOR] â±ï¸ Bilateral filter: {time.time() - t0:.2f}s")
        
        # Step 2: Optional median filter (remove salt-and-pepper noise)
        t0 = time.time()
        if blur_kernel > 0:
            kernel_size = blur_kernel if blur_kernel % 2 == 1 else blur_kernel + 1
            print(f"[IMAGE_PROCESSOR] Applying median blur (kernel={kernel_size})...")
            rgb_processed = cv2.medianBlur(rgb_processed, kernel_size)
        else:
            print(f"[IMAGE_PROCESSOR] Median blur disabled (kernel=0)")
        print(f"[IMAGE_PROCESSOR] â±ï¸ Median blur: {time.time() - t0:.2f}s")
        
        # Step 3: Skip sharpening to prevent noise amplification
        # Sharpening creates high-contrast noise in flat color areas
        print(f"[IMAGE_PROCESSOR] Skipping sharpening to reduce noise...")
        rgb_sharpened = rgb_processed
        
        # Step 4: K-Means quantization with pre-scaling optimization
        h, w = rgb_sharpened.shape[:2]
        total_pixels = h * w
        
        # æ–¹æ¡ˆ 3ï¼šé¢„ç¼©æ”¾ä¼˜åŒ–
        # å¦‚æœåƒç´ æ•°è¶…è¿‡ 50 ä¸‡ï¼Œå…ˆç¼©å°åš K-Meansï¼Œå†æ˜ å°„å›åŸå›¾
        KMEANS_PIXEL_THRESHOLD = 500_000
        
        t0 = time.time()
        if total_pixels > KMEANS_PIXEL_THRESHOLD:
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼Œç›®æ ‡ 50 ä¸‡åƒç´ 
            scale_factor = np.sqrt(total_pixels / KMEANS_PIXEL_THRESHOLD)
            small_h = int(h / scale_factor)
            small_w = int(w / scale_factor)
            
            print(f"[IMAGE_PROCESSOR] ğŸš€ Pre-scaling optimization: {w}Ã—{h} â†’ {small_w}Ã—{small_h} ({total_pixels:,} â†’ {small_w*small_h:,} pixels)")
            
            # ç¼©å°å›¾ç‰‡
            rgb_small = cv2.resize(rgb_sharpened, (small_w, small_h), interpolation=cv2.INTER_AREA)
            
            # åœ¨å°å›¾ä¸Šåš K-Meansï¼ˆä½¿ç”¨ K-Means++ åˆå§‹åŒ–ï¼‰
            pixels_small = rgb_small.reshape(-1, 3).astype(np.float32)
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 50, 0.5)
            flags = cv2.KMEANS_PP_CENTERS  # K-Means++ åˆå§‹åŒ–
            
            t_kmeans = time.time()
            print(f"[IMAGE_PROCESSOR] K-Means++ on downscaled image ({quantize_colors} colors)...")
            _, _, centers = cv2.kmeans(
                pixels_small, quantize_colors, None, criteria, 5, flags
            )
            print(f"[IMAGE_PROCESSOR] â±ï¸ K-Means: {time.time() - t_kmeans:.2f}s")
            
            # ç”¨å¾—åˆ°çš„ centers ç›´æ¥æ˜ å°„åŸå›¾ï¼ˆä¸å†è¿­ä»£ï¼Œåªåšæœ€è¿‘é‚»æŸ¥æ‰¾ï¼‰
            t_map = time.time()
            print(f"[IMAGE_PROCESSOR] Mapping centers to full image...")
            centers = centers.astype(np.float32)
            pixels_full = rgb_sharpened.reshape(-1, 3).astype(np.float32)
            
            # æ‰¹é‡è®¡ç®—æ¯ä¸ªåƒç´ åˆ°æ‰€æœ‰ centers çš„è·ç¦»ï¼Œæ‰¾æœ€è¿‘çš„
            # ä½¿ç”¨ KDTree åŠ é€Ÿ
            from scipy.spatial import KDTree
            centers_tree = KDTree(centers)
            _, labels = centers_tree.query(pixels_full)
            print(f"[IMAGE_PROCESSOR] â±ï¸ KDTree query: {time.time() - t_map:.2f}s")
            
            centers = centers.astype(np.uint8)
            quantized_pixels = centers[labels]
            quantized_image = quantized_pixels.reshape(h, w, 3)
            
            print(f"[IMAGE_PROCESSOR] âœ… Pre-scaling optimization complete!")
        else:
            # å°å›¾ç›´æ¥åš K-Means
            print(f"[IMAGE_PROCESSOR] K-Means++ quantization to {quantize_colors} colors...")
            pixels = rgb_sharpened.reshape(-1, 3).astype(np.float32)
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)
            flags = cv2.KMEANS_PP_CENTERS
            
            _, labels, centers = cv2.kmeans(
                pixels, quantize_colors, None, criteria, 10, flags
            )
            
            centers = centers.astype(np.uint8)
            quantized_pixels = centers[labels.flatten()]
            quantized_image = quantized_pixels.reshape(h, w, 3)
        print(f"[IMAGE_PROCESSOR] â±ï¸ Total quantization: {time.time() - t0:.2f}s")
        
        # [CRITICAL FIX] Post-Quantization Cleanup
        # Removes isolated "salt-and-pepper" noise pixels that survive quantization
        t0 = time.time()
        print(f"[IMAGE_PROCESSOR] Applying post-quantization cleanup (Denoising)...")
        quantized_image = cv2.medianBlur(quantized_image, 3)  # Kernel size 3 is optimal for detail preservation
        print(f"[IMAGE_PROCESSOR] â±ï¸ Post-quantization cleanup: {time.time() - t0:.2f}s")
        
        print(f"[IMAGE_PROCESSOR] Quantization complete!")
        
        # Find unique colors
        t0 = time.time()
        unique_colors = np.unique(quantized_image.reshape(-1, 3), axis=0)
        print(f"[IMAGE_PROCESSOR] Found {len(unique_colors)} unique colors")
        print(f"[IMAGE_PROCESSOR] â±ï¸ Find unique colors: {time.time() - t0:.2f}s")
        
        # Match to LUT
        t0 = time.time()
        print(f"[IMAGE_PROCESSOR] Matching colors to LUT...")
        _, unique_indices = self.kdtree.query(unique_colors.astype(float))
        print(f"[IMAGE_PROCESSOR] â±ï¸ LUT matching: {time.time() - t0:.2f}s")
        
        # ğŸš€ ä¼˜åŒ–ï¼šæ„å»ºé¢œè‰²ç¼–ç æŸ¥æ‰¾è¡¨
        # æŠŠ RGB ç¼–ç æˆå•ä¸ªæ•´æ•°ï¼šR*65536 + G*256 + B
        # è¿™æ ·å¯ä»¥ç”¨ NumPy å‘é‡åŒ–æ“ä½œä¸€æ¬¡æ€§å®Œæˆæ˜ å°„
        t0 = time.time()
        print(f"[IMAGE_PROCESSOR] Building color lookup table...")
        
        # ä¸ºæ¯ä¸ª unique_color è®¡ç®—ç¼–ç 
        unique_codes = (unique_colors[:, 0].astype(np.int32) * 65536 + 
                        unique_colors[:, 1].astype(np.int32) * 256 + 
                        unique_colors[:, 2].astype(np.int32))
        
        # æ„å»ºç¼–ç  â†’ ç´¢å¼•çš„æ˜ å°„æ•°ç»„ï¼ˆç”¨äº np.searchsortedï¼‰
        sort_idx = np.argsort(unique_codes)
        sorted_codes = unique_codes[sort_idx]
        sorted_lut_indices = unique_indices[sort_idx]
        
        # è®¡ç®—æ‰€æœ‰åƒç´ çš„é¢œè‰²ç¼–ç 
        print(f"[IMAGE_PROCESSOR] Mapping to full image (optimized)...")
        flat_quantized = quantized_image.reshape(-1, 3)
        pixel_codes = (flat_quantized[:, 0].astype(np.int32) * 65536 + 
                       flat_quantized[:, 1].astype(np.int32) * 256 + 
                       flat_quantized[:, 2].astype(np.int32))
        
        # ä½¿ç”¨ searchsorted æ‰¾åˆ°æ¯ä¸ªåƒç´ å¯¹åº”çš„ unique_color ç´¢å¼•
        insert_positions = np.searchsorted(sorted_codes, pixel_codes)
        # è·å–å¯¹åº”çš„ LUT ç´¢å¼•
        lut_indices_for_pixels = sorted_lut_indices[insert_positions]
        
        # ä¸€æ¬¡æ€§æ˜ å°„æ‰€æœ‰åƒç´ 
        matched_rgb = self.lut_rgb[lut_indices_for_pixels].reshape(target_h, target_w, 3)
        material_matrix = self.ref_stacks[lut_indices_for_pixels].reshape(
            target_h, target_w, PrinterConfig.COLOR_LAYERS
        )
        print(f"[IMAGE_PROCESSOR] â±ï¸ Color mapping (optimized): {time.time() - t0:.2f}s")
        
        print(f"[IMAGE_PROCESSOR] âœ… Total processing time: {time.time() - total_start:.2f}s")
        
        # Prepare debug data
        debug_data = {
            'quantized_image': quantized_image.copy(),
            'num_colors': len(unique_colors),
            'bilateral_filtered': rgb_processed.copy(),
            'sharpened': rgb_sharpened.copy(),
            'filter_settings': {
                'blur_kernel': blur_kernel,
                'smooth_sigma': smooth_sigma
            }
        }
        
        return matched_rgb, material_matrix, quantized_image, debug_data
    
    def _process_pixel_mode(self, rgb_arr, target_h, target_w):
        """
        Pixel art mode image processing
        Direct pixel-level color matching, no smoothing
        """
        print(f"[IMAGE_PROCESSOR] Direct pixel-level matching (Pixel Art mode)...")
        
        flat_rgb = rgb_arr.reshape(-1, 3)
        _, indices = self.kdtree.query(flat_rgb)
        
        matched_rgb = self.lut_rgb[indices].reshape(target_h, target_w, 3)
        material_matrix = self.ref_stacks[indices].reshape(
            target_h, target_w, PrinterConfig.COLOR_LAYERS
        )
        
        print(f"[IMAGE_PROCESSOR] Direct matching complete!")
        
        return matched_rgb, material_matrix, rgb_arr
